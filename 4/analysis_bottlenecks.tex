% -*- root: ../main.tex -*-
%!TEX root = ../main.tex
% this file is called up by main.tex
% content in this file will be fed into the main document
% vim:textwidth=80 fo=cqt


The  Reduced  Order  Model  (ROM)  obtained   by  Dr.  Lee  is  based  upon  the
thermodynamic and kinetic equations of the one-dimensional volume-averaged model
first  proposed  by Doyle,  Fuller  and  Newman  (cite). The  overall  modelling
procedure is summarized  in Figure 1. First, the model  equations are linearized
about an operating point  and closed-form Laplace-domain transcendental transfer
functions of  all the  internal cell  variables(phi\_s,phi\_e,c\_s,c\_e \&  j at
different cell  locations)are derived using the  applied current to the  cell as
input.  The transfer  functions  in (cite)  are summarized  in  Appendix I.  The
authors proposed a  novel Discrete-time-Realization algorithm (DRA)  in (DRA) to
transform  these  transcendental transfer  functions  to  equations in  standard
state-space representation. The steps  involved in the Discrete-Time Realization
Algorithm (DRA) is summarized  in the block diagram (Figure 2).  At the heart of
this method is  the classical subspace identification method  known as Ho-Kalman
algorithm (cite  2 papers)  (visualized in Figure  3). A key  step in  the model
order reduction process is the  Singular Value Decomposition of the Block-Hankel
matrix formed  by the  Markov parameters (unit-pulse  responses) of  the battery
transfer functions. In  the illustrative example provided by Dr.  Lee, there are
28 transfer functions, with  applied current as the input, ie.  we have a Single
input Multiple Output System (SIMO), the Markov parameters of which is collected
into a  Block-Hankel matrix  as shown  in Figure  x. Each  Block element  in the
Hankel matrix  is a column vector  comprising of the Markov  parameters at every
time-step. Singular Value Decomposition (SVD)  is performed on this Block-Hankel
matrix and plotted as a visual aid  to provide an insight into the system order.
A  detailed analysis  of this  procedure reveals  certain weaknesses,  issuesand
inefficiencies in this SVD computation step as discusses below.

\subsection{Size of the Block-Hankel Matrix}

The SVD  computation can become  intractable if  the Hankel matrix  becomes very
large.  For  Li-ion  battery  modelling,  this happens  due  to  the  following.
\begin{enumerate} \item For a desired duration of markov-parameter recording for
the SIMO system,  if the model's sampling frequency is  increased, the emulation
frequency F1 has to be proportionately increased for the interpolation in step 3
to be  accurate. This requires  that the total  number of time-samples  for each
Markov parameter (N) to be scaled proportionately to capture the duration of the
unit-pulse-response.  However,  the  number  of entries  in  the  Hankel  matrix
increases  as  per a  quadratic  law.  \item  For  a battery  modelling  problem
consisting  of multiple  transfer functions  (28 in  this case),  the number  of
entries  in the  Hankel  matrix also  scales linearly  with  number of  transfer
functions. Thus, if more transfer  functions (say, concentrations and potentials
at different locations  in the cell) need  to be modelled, then the  size of the
Block-Hankel  matrix increases  correspondingly. \end{enumerate}  Combining both
the effects above, if  x transfer functions are to be modelled  and N samples of
markov parameters are to  be captured for the duration of  interest, the size of
the Block-Hankel matrix would be
\[
Size(H)\sim O(xN^{2})
\]
The  size  of the  Hankel  matrix  has  a  significant computational  impact  as
described in the following sections.

\subsubsection{Effect on Computational Demand}

If the unit-pulse-response of just any one of the transfer functions of the SIMO
system  does  not die  down  to  zero in  a  reasonable  duration, the  recorded
sample  size  for  each  Markov  parameter, N  becomes  very  large.  In  Li-ion
batteries, diffusion within  the solid particle is considered to  be the slowest
ie.rate-determining  step. In  the case  of the  cell modelled  by Dr.  Lee, the
Markov  parameters of  the  surface concentration  of Li  next  to the  positive
current collector dies out very slow as shown in figure x.

\subsubsection*{Analysis of Memory (RAM) Requirements for computing SVD}

Dr. Lee chose  to use 8000 samples  for his reduced order model  with a sampling
interval of 1 second to allow settling  of the solid surface concentration to an
acceptably low  magnitude. The size  of the  Block-Hankel matrix thus  formed is
224000  x 8000.  Using double-precision  arithmetic  (4 bytes  on most  computer
architecture),  the storage  requirement  for this  Block-hankel  matrix can  be
estimated as
\[
MemoryRequiredforholdingHankelMatrix=4x28x(8000)^{2}bytes=7168000000bytes=6.67GB
\]

However,  as  seen in  the  unit-pulse-response  (easrlier figure),  the  Markov
parameter does not quite die down  to zero. Recording the Markov parameters till
16000  seconds  would require  about  27GB  RAM for  storage.Furthermore,  after
computing the SVD, three more large  matrices $U,Vand\Sigma$of the same size are
formed  in memory.  Thus, running  the algorithm  necessitates a  requirement of
26.68 GB of free memory. For  calculating the system dynamics matrix A\_hat, the
row-shifted Hankel matrix  (symbol here) is also held in  memory (although to be
fair, only the first  n number of columns of U, V and  Sigma are retained before
this computation,  and the original  unshifted Hankel  matrix can be  cleared to
free up  RAM) is further  compounded by the  necessity of more  higher frequency
models.  For using  the reduced  order  model succesfully  in high-dynamic  load
drive-cycles (like cite eg.), it requires  that the model be valid atleast until
the highest  operating frequency of  the drive-cycle. Assuming  no computational
errors, the  sampling frequency required  to perfectly capture all  the dynamics
will be  atleast twice of this  nyquist frequency. For the  UDDS drive-cyle, the
highest frequency  conent of the  input current is.  Thus, if the  reduced order
model (ROM) running  at 0.5s (verify) sample  time is to be  derived, then 32000
time-samples  are needed  for  each  Markov parameter  for  capturing the  pulse
duration  until  16000  seconds.  Analysis  of  the  memory  calculation  yields
$4x28x(32000)^{2}$bytes  = 114688000000  byes  =  106.8 GB  of  free memory  for
holding  the Hankel  matrix alone,  and a  total of  427 GB  of free  memory for
holding the  resulting output matrices.  Intermediate memory storage  during the
SVD computation and the baseline operating  memory are not taken into account in
this calculation.  The problem is further  compounded by having a  battery model
wherein the smaller diffusion coefficients  are smaller than those studied here.
The Markov parameters of the solid surface concentration transfer function shall
now decay to  zero further slowly, necessitating a very  large number of samples
to be  captured. Furthermore,obtaining the  battery model at different  SOCs and
temperature would  mean that  we need  to re-linearize  the original  PDEs under
those  conditions  and  obtain  the  corresponding  transfer  functions,  markov
parameters and  Block-Hankel matrices.  These large matrices  have to  be loaded
onto the memory for every SOC  and temperature run, thereby signficantly slowing
down the modelling procedure. This analysis  reveals that doubling the number of
transfer functions doubles the size of the Block-Hankel matrix.

\subsubsection*{Analysis of CPU Operation Count for computing SVD}

The most widely  used numerical algorithm for computing the  full Singular Value
Decomposition (SVD) of a  general dense matrix $A\epsilon\mathcal{R}^{mxn},m\geq
n$ is the  Golub-Kahan-Reinsch method (cite). The first  stage imvolves reducing
the dense  matrix $A\epsilon\mathcal{R}^{mxn},$into  an upper  bidiagonal matrix
$B\epsilon\mathcal{R}^{mxn}$by  the standard  bidiagonalisation algorithm  using
Householder reflections. This  algorithm requires $4mn^{2}-4n^{3}/3$ operations.
However,  in the  case of  battery modelling  problem, we  always have  a 'tall'
Block-Hankel matrix, since the number of time-samples of the unit-pulse response
collected is far greater than the  number of transfer functions being modelled.,
ie.  $m\gg  n$.  Furthermore,  there  exists an  efficient  algorithm  for  this
step  if  $m\geq5n/3$  known as  $\mathcal{R}$-Bidiagonalisation  (cite),  which
first  reduces the  matrix $A\epsilon\mathcal{R}^{mxn}$  to a  triangular matrix
using  the QR\LyXFourPerEmSpace  decomposition  and  then employing  Householder
reflections  to  further reduce  the  matrix  to  bidiagonal form.  Again,  this
condition  of  $m\geq5n/3$  is  always satisfied  for  the  Block-Hankel  matrix
formed  by  the  markov parameters  of  a  vector  of  transfer functions  of  a
typical  battery. The  operation count  for the  $\mathcal{R}$-Bidiagonalisation
step  is $2mn^{2}+2n^{3}$.  The  second  stage computes  the  SVD  of the  upper
bidiagonal  matrix  $B\epsilon\mathcal{R}^{mxn}$  using an  iterative  procedure
(Demmel-Kahan method)  upto a certain  precision, typically the  machine epsilon
(cite).  The   second  stage  takes  $\mathcal{O}(n)$iterations,   each  costing
$\mathcal{O}(n)$ floating point  operations.This is typically done  by a variant
of  the  QR\LyXFourPerEmSpace  algorithm  for the  computation  of  eigenvalues,
which  was  first   described  by  Golub\LyXFourPerEmSpace  \&\LyXFourPerEmSpace
Kahan\LyXFourPerEmSpace  (1965). The  LAPACK subroutine  DBDSQR implements  this
iterative  method,  with  some  modifications   to  cover  the  case  where  the
singular  values are  very small  (Demmel\LyXFourPerEmSpace \&\LyXFourPerEmSpace
Kahan\LyXFourPerEmSpace  1990).Together  with  a first  step  using  Householder
reflections and, if appropriate, QR decomposition, this forms the DGESVD routine
for the computation  of the singular value decomposition for  a real rectangular
matrix. Other  variants of  this routine are  available including  DGESDD (which
uses a divide-and-conquer algorithm for the  bidiagonal SVD) and ZGESDD (for SVD
of a complex matrix). The DSESVD  algorithm, originally implemented in LAPACK is
numerically  stable and  versitile has  ported onto  many numerical  computation
packages (MATLAB,GNU  Octave, Scilab) and  numerical libraries (NAG,  Intel MKL)
that this is the de-facto SVD algorithm nowadays . The MATLAB implementation svd
is also based upon this.

If $\mathcal{R}$-Bidiagonalisation for stage I  of the SVD computation, then the
overall  process  is referred  to  as  $\mathcal{R}-SVD.,$which is  the  fastest
standard  algorithm for  the  full-SVD computation  for  this battery  modelling
problem at hand. The exact analysis of the overall operation count for computing
the singular values and singular  vectors using the $\mathcal{R}-SVD$ method was
analysed in  (cite) and  is shown  to be  $4m^{2}n+22n^{3}$. For  a Block-Hankel
matrix constructed from the Markov parameters  of $x$ transfer functions and $N$
time-samples of pulse-response data for  each transfer functions, we have $m=xN$
(rows) and $n=$N (columns)

\[
ExactOperationCount=4(xN)^{2}N+22N^{3}=2N^{3}(11+2x^{2})
\]

For the  battery modelling task  at hand,  consisting of 28  transfer functions,
Markov  parameters  are  collected  for  a duration  of  8000  seconds  using  a
sampling interval of  1second. Thus the operation count of  SVD is approximately
$O(8000^{3})=1.6169e+15floating$point operations.  On the latest  6th generation
Quad-Core Intel  Core i7-6700K desktop processor  (Skylake architecture, 4.2GHz,
8M cache, released August 2015)  capable of 81.28 GFlops (Whetstone Double-Float
Benchmark), even with implicit paralleization,this SVD operation procedure would
theoretically  take 1.9893e+04  seconds =  331.5486  minutes =  5.525 hours.  To
obtain the reduced-order battery model for  a wide SOCs and temperatures we need
to  re-lineare the  original PDE  equations  for these  conditions (cite).  This
implies that  the SVD  has to be  re-computed for the  new set  of . For  20 SOC
points  from 0  to  100\%  and for  a  temperature range  from  -10C  to 40C  at
increments of 10C, there  are a total of 120 operating  points, which means that
663  hours  of  operation  on  a high-end  machine.  The  SVD  computation  time
scales  up  as the  cube  of  the chosen  number  of  time-samples of  the  unit
pulse-response,  rendering  calculations for  anything  more  than 8000  samples
virtually  intractable,  which would  automatically  preclude  choosing a  lower
sampling frequency of  the model and inherently prevents  capturing the dynamics
of drive-cyles  above 2Hz  (for this  specific case  under consideration  with 1
second sampling frequency).  Mention about 16-core workstation  and estimate the
hours of SVD operation for a  single SOC and temperature. Choosing more transfer
functions to  study has  a quadratic  increase on the  number of  floating point
operations. Thus doubling  the number of transfer functions  being modelled will
effectively require about 22 hours on  the quad-core machine and \_\_\_ hours on
the 16-core workstation  for computing the SVD at a  single SOC and temperature.
Although a  GPU farm computation of  the SVD using CUDA/OpenCL  libraries can be
considered,  large scale  data  movement quickly  overwhems  the GPU  pipelines.
Furthermore, even  the latest  NVIDIA Tesla  high-end scientific  computing GPUs
cannot handle  the sheer amount of  memory required even for  a moderately sized
problem of 28 battery transfer functions and 8000 samples.

From the above discussion, the SVD computation  step in the DRA procedure can be
identified as Bottleneck \#1.

\subsubsection*{Analysis of Memory for Computation of System Dynamics Matrix}

Verify on my computer if this is indeed a problem or not (whether we can get the
same A by either method ?.

The computation of the system dynamics matrix is computed by

\[
\hat{A}=\mathcal{O}_{l}^{\dagger}\mathcal{O}_{l+1}^{\uparrow}
\]

where  $O_{l}$is  the  extended  observability   matrix  of  the  system  formed
using  a   Block-Hankel  matrix  with  $l$   rows.  This  can  be   computed  as
$\mathcal{O}_{l}=U_{1}\Sigma_{1}^{1/2}$,  where  $\Sigma_{1}$  is  the  diagonal
matrix  formed   by  keeping  only  the   first  $n$  singular  values   of  the
full   set   of   singular   values  obtained   through   the   Singular   Value
Decomposition. $U_{1}$represents  the first $n$  columns of the  full orthogonal
matrix  $U$.   $O_{l}^{\dagger}$represents  the   Moore-Penrose  pesudo-inverse.
$O_{l+1}^{\uparrow}$represents the  extended observability matrix of  the system
formed by row-shifting by one row.

However, this process  requires the computationally expensive  step of computing
the  Singular Value  Decomposition step  twice  for determing  the two  extended
observability matrices.  First, the SVD of  a Block-Hankel matrix with  $l$ rows
has to be performed. Then, for computing the

\subsubsection{Summary Effect of Computational Bottlenecks}

Although the analytical framework of  the reduced order model formulation allows
the  user  to simulate  the  cell-variables  at  any  location of  interest,  in
practice, this  capability is severely hampered  by the bottlenecks in  the DRA.
The real-life scenarios enumerated below highlights the significant reduction in
the general  applicability of this modelling  approach.. \begin{enumerate} \item
The linear increase in memory requirement  with the number of transfer functions
quickly limits  the number of  transfer functions  can be studied,  eg. Studying
cell-variables at other locations, that may  be of interest within the cell, eg.
in  the middle  of  the  electrode thicknesses  becomes  intractable. \item  The
quadratic  increase in  the  SVD operation  count with  the  number of  transfer
functions  greatly  reduces  the  computational speed.  This  inhibits  a  quick
'what-if' scenario analysis,  eg. monte-carlo sweep to understand  the effect of
just  physical parameter  like  the diffusion  coeffient  or conductivity  would
require the  user to wait  for hours on-end at  the terminal, defeating  the end
gains of the model order reduction  process \item The extensibility of the model
will significantly slow  down in studying battery systems  of lower diffusivity,
eg. in the  case of Li-S batteries, the precipitation  step is the rate-limiting
step  and the  Markov  parameters die  down  after a  very  long duration.  This
necessitates  early  truncation of  the  Markov  parameters to  avoid  computing
bottlenecks and  hence, resulting in severe  modelling errors (due to  errors in
the singular  values and vectors)  \end{enumerate} The large  memory requirement
also means  that the modelling  process is not accessible  to a large  number of
research groups  without specialized  high-memory computing  infrastucture. Even
with access  to a  computing machine  with high  memory and  powerful processor,
the  modelling  process  takes  hours  of  computation  for  a  single  SOC  and
temperature.Repeatng  the process  across  various SOCs  and temperatures  would
naturally slow down the whole modelling procedure.

In  the next  sections, we  show an  improved method  that retains  the powerful
physics-based reduced order modelling based that replaces the key bottlenecks in
the DRA with highly efficient computational steps.

